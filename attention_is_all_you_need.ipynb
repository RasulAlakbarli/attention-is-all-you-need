{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention is all you need\n",
    "\n",
    "### Rasul Alakbarli, Mahammad Nuriyev, Petko Petkov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import math\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General `Module` class so we always have access to the device used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module(nn.Module):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the embeddings in the transformer architecture (both `input` and `output` with the positional encodings):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(Module):\n",
    "    def __init__(self, d_model, vocab_len, pad_index, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.embedding = nn.Embedding(vocab_len, self.d_model, padding_idx=pad_index)\n",
    "        self.dropout = nn.Dropout(p=dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Embedding shape: (batch, sequence_len, d_model)\n",
    "        # Positional encoding shape: (sequence_len, d_model)\n",
    "        return self.dropout(self.embedding(x) + self.positional_encoding(x))\n",
    "\n",
    "    def positional_encoding(self, x):\n",
    "        # result.shape = (seq_len, d_model)\n",
    "        result = torch.zeros(\n",
    "            (x.size(1), self.d_model),\n",
    "            dtype=torch.float,\n",
    "            requires_grad=False\n",
    "        )\n",
    "\n",
    "        # pos.shape = (seq_len, 1)\n",
    "        pos = torch.arange(0, x.size(1)).unsqueeze(1)\n",
    "\n",
    "        # dim.shape = (d_model)\n",
    "        dim = torch.arange(0, self.d_model, step=2)\n",
    "\n",
    "        # Sine for even positions, cosine for odd dimensions\n",
    "        result[:, 0::2] = torch.sin(pos / (10_000 ** (dim / self.d_model)))\n",
    "        result[:, 1::2] = torch.cos(pos / (10_000 ** (dim / self.d_model)))\n",
    "        return result.to(self.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation of feed-forward neural network which is used in both of the encoder and decoder parts in the transformer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNetwork(Module):\n",
    "    def __init__(self, d_model, d_ff=2048):\n",
    "        super().__init__()\n",
    "\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear2(self.relu(self.linear1(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Multi-Head Attention` module of the transformer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(Module):\n",
    "    def __init__(self, d_model, num_heads=8, use_mask=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.use_mask = use_mask\n",
    "        assert d_model % num_heads == 0, 'D_MODEL must be divisible by NUM_HEADS'\n",
    "\n",
    "        # w_q_i projects D_MODEL to D_MODEL / NUM_HEADS. However, there are\n",
    "        # NUM_HEADS parallel attention layers that are concatenated, so in the\n",
    "        # end output dim is still D_MODEL / NUM_HEADS * NUM_HEADS = D_MODEL\n",
    "        self.w_q = nn.Linear(d_model, d_model)\n",
    "        self.w_k = nn.Linear(d_model, d_model)\n",
    "        self.w_v = nn.Linear(d_model, d_model)\n",
    "        self.w_o = nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, queries, keys, values):\n",
    "        # queries, keys, values = (batch, seq, 512)\n",
    "        # w_q = (512, 512)\n",
    "        # queries @ w_q.t = (batch, seq, 512)\n",
    "        # split_heads = (batch, 8, seq, 64)\n",
    "        q = self.split_heads(self.w_q(queries))\n",
    "        k = self.split_heads(self.w_k(keys))\n",
    "        v = self.split_heads(self.w_v(values))\n",
    "\n",
    "        # Perform NUM_HEADS parallel single-head attention\n",
    "        attention = self.scaled_dot_product_attention(q, k, v)\n",
    "\n",
    "        # Concatenate and return multi-headed results\n",
    "        # (batch, 8, seq, 64) -> (batch, seq, 512)\n",
    "        merged = self.merge_heads(attention)\n",
    "\n",
    "        # Apply final projection matrix\n",
    "        return self.w_o(merged)\n",
    "\n",
    "    def split_heads(self, x):\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "\n",
    "        # Split D_MODEL into NUM_HEADS channels of D_MODEL // NUM_HEADS each\n",
    "        # Now shape is (batch, seq, num_heads, d_model/num_heads)\n",
    "        heads = x.reshape(batch_size, seq_len, self.num_heads, self.d_model // self.num_heads)\n",
    "\n",
    "        # However, we want (batch, num_heads, seq, d_model/num_heads) because each tensor\n",
    "        # of size (seq, d_model/num_heads) represents a single-head attention\n",
    "        return heads.transpose(2, 1)\n",
    "\n",
    "    def merge_heads(self, x):\n",
    "        # Concatenate multi-headed results back into shape (batch, seq, d_model)\n",
    "        # This is the inverse of split_heads\n",
    "        batch_size, _, seq_len, _ = x.size()\n",
    "\n",
    "        # Switch back to shape (batch, seq, num_heads, d_model)\n",
    "        transposed = x.transpose(1, 2)\n",
    "\n",
    "        # Merge last two dimensions\n",
    "        return transposed.reshape(batch_size, seq_len, self.d_model)\n",
    "\n",
    "    def scaled_dot_product_attention(self, q, k, v):\n",
    "        # Inputs are size (batch, num_heads, seq, d_model/num_heads)\n",
    "        d_k = self.d_model // self.num_heads\n",
    "        compatibility = torch.matmul(q, k.transpose(2, 3)) / math.sqrt(d_k)\n",
    "\n",
    "        \"\"\"\n",
    "        Use lower-triangular mask to prevent leftward information flow\n",
    "        Fill upper triangle with negative infinity to zero out those values during softmax\n",
    "\n",
    "        seq     weights      values          output\n",
    "        0       [1 0 0]   [ --- a --- ]   [ a + 0 + 0 ]\n",
    "        1       [1 1 0] * [ --- b --- ] = [ a + b + 0 ]\n",
    "        2       [1 1 1]   [ --- c --- ]   [ a + b + c ]\n",
    "\n",
    "        At seq=0, can only attend to seq=0\n",
    "        At seq=1, can attend to both seq=0 and seq=1\n",
    "        And so on...\n",
    "        \"\"\"\n",
    "        if self.use_mask:\n",
    "            seq_len = compatibility.size(-1)\n",
    "            mask = torch.triu(  # Prevents leftward flow of information in target seq\n",
    "                torch.ones(seq_len, seq_len, dtype=torch.bool, requires_grad=False),\n",
    "                diagonal=1\n",
    "            ).to(self.device)\n",
    "            compatibility = torch.masked_fill(compatibility, mask, float('-inf'))\n",
    "\n",
    "        # Apply softmax along the last dimension\n",
    "        value_weights = self.softmax(compatibility)\n",
    "\n",
    "        # Weight values by softmax results\n",
    "        return torch.matmul(value_weights, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have all of the required parts to complete the encoder and decoder parts of the architecture which contains an `encoder` and a `decoder`.\n",
    "\n",
    "Implementation of the `Encoder`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(Module):\n",
    "    def __init__(self, d_model, num_heads=8, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.self_attention = MultiHeadAttention(d_model, num_heads=num_heads)\n",
    "        self.dropout1 = nn.Dropout(p=dropout_rate)\n",
    "        self.layer_norm1 = nn.LayerNorm(d_model)\n",
    "\n",
    "        self.ffn = FeedForwardNetwork(d_model)\n",
    "        self.dropout2 = nn.Dropout(p=dropout_rate)\n",
    "        self.layer_norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Multi-headed attention and residual connection + layer norm\n",
    "        # Dropout is applied to sub-layer output, before residual and norm\n",
    "        attention_out = self.self_attention(queries=x, keys=x, values=x)\n",
    "        x = self.layer_norm1(x + self.dropout1(attention_out))\n",
    "\n",
    "        # Feed-forward network and another residual + layer norm\n",
    "        ffn_out = self.ffn(x)\n",
    "        return self.layer_norm2(x + self.dropout2(ffn_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Decoder` is very similar to the `Encoder`. The main differences are that its input is the output of the `Encoder` and it includes a `Linear` and a `Softmax` layer at the end to make the final prediction.\n",
    "\n",
    "Implementation of the `Decoder`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(Module):\n",
    "    def __init__(self, d_model, num_heads=8, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.self_attention = MultiHeadAttention(d_model, num_heads=num_heads, use_mask=True)\n",
    "        self.dropout1 = nn.Dropout(p=dropout_rate)\n",
    "        self.layer_norm1 = nn.LayerNorm(d_model)\n",
    "\n",
    "        self.enc_attention = MultiHeadAttention(d_model, num_heads=num_heads)\n",
    "        self.dropout2 = nn.Dropout(p=dropout_rate)\n",
    "        self.layer_norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "        self.ffn = FeedForwardNetwork(d_model)\n",
    "        self.dropout3 = nn.Dropout(p=dropout_rate)\n",
    "        self.layer_norm3 = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x, enc_out):\n",
    "        # Multi-headed attention and residual connection + layer norm\n",
    "        attention_out = self.self_attention(queries=x, keys=x, values=x)\n",
    "        x = self.layer_norm1(x + self.dropout1(attention_out))\n",
    "\n",
    "        # Multi-headed attention over output of encoder stack\n",
    "        # Use ENC_OUT as the keys and values, the queries come from previous attention\n",
    "        # Values come from encoder, so need to use encoder mask for this attention\n",
    "        attention_out = self.enc_attention(queries=x, keys=enc_out, values=enc_out)\n",
    "        x = self.layer_norm2(x + self.dropout2(attention_out))\n",
    "\n",
    "        # Feed-forward network and another residual + layer norm\n",
    "        ffn_out = self.ffn(x)\n",
    "        return self.layer_norm3(x + self.dropout3(ffn_out))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have all of the modules required and we can combine them into a final class to get the full transformer architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(Module):\n",
    "    def __init__(self,\n",
    "                 d_model,\n",
    "                 src_vocab_len,\n",
    "                 trg_vocab_len,\n",
    "                 src_pad_index,\n",
    "                 trg_pad_index,\n",
    "                 num_heads=8,\n",
    "                 num_layers=6,\n",
    "                 dropout_rate=0.1,\n",
    "                 seed=20230815):\n",
    "        super().__init__()\n",
    "\n",
    "        self.src_pad_index = src_pad_index\n",
    "        self.trg_pad_index = trg_pad_index\n",
    "\n",
    "        # Manually seed to keep embeddings consistent across loads\n",
    "        torch.manual_seed(seed)\n",
    "\n",
    "        # Embeddings, pass in pad indices to prevent <pad> from contributing to gradient\n",
    "        self.src_embedding = Embedding(d_model,\n",
    "                                       src_vocab_len,\n",
    "                                       src_pad_index,\n",
    "                                       dropout_rate=dropout_rate)\n",
    "        self.trg_embedding = Embedding(d_model,\n",
    "                                       trg_vocab_len,\n",
    "                                       trg_pad_index,\n",
    "                                       dropout_rate=dropout_rate)\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder_stack = nn.ModuleList(\n",
    "            [EncoderLayer(d_model,\n",
    "                          num_heads=num_heads,\n",
    "                          dropout_rate=dropout_rate)\n",
    "             for _ in range(num_layers)]\n",
    "        )\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder_stack = nn.ModuleList(\n",
    "            [DecoderLayer(d_model,\n",
    "                          num_heads=num_heads,\n",
    "                          dropout_rate=dropout_rate)\n",
    "             for _ in range(num_layers)]\n",
    "        )\n",
    "\n",
    "        # Final layer to project embedding to target vocab word probability distribution\n",
    "        self.linear = nn.Linear(d_model, trg_vocab_len)\n",
    "\n",
    "        # Move to GPU if possible\n",
    "        self.to(self.device)\n",
    "\n",
    "        # Re-seed afterward to allow shuffled data\n",
    "        torch.seed()\n",
    "\n",
    "    def forward(self, source, target):\n",
    "        # Encoder stack\n",
    "        enc_out = self.src_embedding(source)\n",
    "        for layer in self.encoder_stack:\n",
    "            enc_out = layer(enc_out)\n",
    "\n",
    "        # Decoder stack\n",
    "        dec_out = self.trg_embedding(target)\n",
    "        for layer in self.decoder_stack:\n",
    "            dec_out = layer(dec_out, enc_out)\n",
    "\n",
    "        # Final linear layer to get word probabilities\n",
    "        # DO NOT apply softmax here, as CrossEntropyLoss already does softmax!!!\n",
    "        return self.linear(dec_out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
